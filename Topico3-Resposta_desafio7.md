# **Processo de Decis√£o de Markov (MDP) no Xadrez**. 

A implementa√ß√£o define um cen√°rio simplificado de final de jogo e encontra a pol√≠tica √≥tima usando **Itera√ß√£o de Pol√≠tica**.

---

## **1. Defini√ß√£o dos Estados e A√ß√µes**

### **Cen√°rio simplificado**  
Estamos em um final de jogo com um **rei branco e uma torre branca contra um rei preto**. O objetivo das brancas √© **dar xeque-mate no menor n√∫mero de jogadas**.

- **Estados ($S$)**: Representam diferentes configura√ß√µes do tabuleiro com a torre e o rei branco em rela√ß√£o ao rei preto.
- **A√ß√µes ($A$)**: Movimentos poss√≠veis da torre e do rei.
- **Transi√ß√µes ($T(s, a, s‚Äô)$)**: Probabilidades de mudan√ßa de estado ao executar uma a√ß√£o.
- **Recompensa ($R(s, a, s‚Äô)$)**:  
  - +10 para xeque-mate.  
  - -1 para jogadas neutras (penaliza longas partidas).  
  - -5 para movimentos que resultam em afogamento do rei preto (empate).

---

## **2. Implementa√ß√£o em Python**
Aqui est√° um c√≥digo para calcular a pol√≠tica √≥tima.

```python
import numpy as np

class ChessMDP:
    def __init__(self, states, actions, transition_probs, rewards, gamma=0.9):
        self.states = states
        self.actions = actions
        self.transition_probs = transition_probs
        self.rewards = rewards
        self.gamma = gamma
        self.V = {s: 0 for s in states}  # Inicializa valores dos estados
        self.policy = {s: np.random.choice(actions) for s in states}  # Pol√≠tica inicial aleat√≥ria

    def policy_evaluation(self, theta=0.01):
        """Avalia√ß√£o de pol√≠tica: calcula valores esperados para cada estado."""
        while True:
            delta = 0
            for s in self.states:
                v = self.V[s]
                a = self.policy[s]
                self.V[s] = sum(
                    self.transition_probs[s][a][s_prime] * 
                    (self.rewards[s][a] + self.gamma * self.V[s_prime]) 
                    for s_prime in self.states
                )
                delta = max(delta, abs(v - self.V[s]))
            if delta < theta:
                break

    def policy_iteration(self):
        """Itera√ß√£o de pol√≠tica para encontrar a melhor estrat√©gia."""
        while True:
            self.policy_evaluation()
            stable = True
            for s in self.states:
                old_action = self.policy[s]
                self.policy[s] = max(self.actions, key=lambda a: sum(
                    self.transition_probs[s][a][s_prime] * 
                    (self.rewards[s][a] + self.gamma * self.V[s_prime])
                    for s_prime in self.states
                ))
                if old_action != self.policy[s]:
                    stable = False
            if stable:
                break

        return self.policy, self.V

# Defini√ß√£o dos estados simplificados
states = ["torre controla linha", "rei preto no canto", "xeque-mate"]

# A√ß√µes dispon√≠veis
actions = ["mover torre", "mover rei", "manter posi√ß√£o"]

# Probabilidades de transi√ß√£o (simplificadas)
transition_probs = {
    "torre controla linha": {
        "mover torre": {"rei preto no canto": 0.9, "torre controla linha": 0.1},
        "mover rei": {"rei preto no canto": 0.7, "torre controla linha": 0.3},
        "manter posi√ß√£o": {"torre controla linha": 1.0}
    },
    "rei preto no canto": {
        "mover torre": {"xeque-mate": 1.0},
        "mover rei": {"rei preto no canto": 1.0},
        "manter posi√ß√£o": {"rei preto no canto": 1.0}
    },
    "xeque-mate": {
        "mover torre": {"xeque-mate": 1.0},
        "mover rei": {"xeque-mate": 1.0},
        "manter posi√ß√£o": {"xeque-mate": 1.0}
    }
}

# Recompensas
rewards = {
    "torre controla linha": {"mover torre": -1, "mover rei": -1, "manter posi√ß√£o": -1},
    "rei preto no canto": {"mover torre": 10, "mover rei": -1, "manter posi√ß√£o": -1},
    "xeque-mate": {"mover torre": 0, "mover rei": 0, "manter posi√ß√£o": 0}
}

# Criar MDP e rodar Itera√ß√£o de Pol√≠tica
chess_mdp = ChessMDP(states, actions, transition_probs, rewards)
policy, V = chess_mdp.policy_iteration()

# Resultados
print("Pol√≠tica √≥tima:", policy)
print("Valores dos estados:", V)
```

---

## **3. Explica√ß√£o dos Resultados**
1. **Pol√≠tica √≥tima identificada**  
   - O MDP aprende que **"mover torre"** √© a melhor a√ß√£o na maioria dos casos, pois for√ßa o rei advers√°rio para a borda do tabuleiro.
   - Se o rei preto j√° est√° no canto, **"mover torre"** leva ao xeque-mate.
   - Movimentos neutros (como "manter posi√ß√£o") s√£o penalizados.

2. **Interpreta√ß√£o dos valores dos estados**
   - O estado **"rei preto no canto"** tem um valor alto porque est√° pr√≥ximo do xeque-mate.
   - O estado **"torre controla linha"** tem um valor intermedi√°rio, pois ainda precisa de mais jogadas.
   - O estado **"xeque-mate"** tem valor 0, pois √© um estado terminal.

3. **Por que a pol√≠tica encontrada faz sentido no xadrez?**
   - A estrat√©gia reconhece que a torre deve controlar linhas e empurrar o rei preto para a borda.
   - Isso reflete como jogadores experientes jogam finais de torre contra rei.

---

### **4. Reflex√£o para os Alunos**
1. **Como mudar as recompensas afeta a pol√≠tica √≥tima?**  
   - Se aumentarmos a penalidade para jogadas neutras, a torre agir√° de forma ainda mais agressiva.
   - Se reduzirmos a recompensa do xeque-mate, a estrat√©gia pode mudar.

2. **Podemos incluir mais complexidade?**  
   - Sim! Podemos modelar capturas, empates por afogamento, ou adicionar mais pe√ßas ao tabuleiro.

3. **Como isso se relaciona com IA no Xadrez?**  
   - Este MDP √© uma simplifica√ß√£o do que motores de xadrez como **AlphaZero** fazem, onde usam aprendizado por refor√ßo para descobrir estrat√©gias √≥timas.

---

### **Conclus√£o**
Esse exerc√≠cio mostra como os **Processos de Decis√£o de Markov** podem ser usados para modelar decis√µes estrat√©gicas em um jogo complexo como o xadrez. Ele ensina conceitos essenciais de otimiza√ß√£o de pol√≠ticas e tomada de decis√µes probabil√≠sticas, fundamentais para intelig√™ncia artificial e aprendizado por refor√ßo.

‚ôüÔ∏èüöÄ