{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-13jda43bE2XdiIkcqHS5_RP8F7fx1Nm","timestamp":1743549345940}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Algoritmo que usa **Markov Decision Processes (MDP)** para **Reinforcement Learning (RL)**. Vamos implementar um **Agente de Iteração de Valor** para resolver um MDP simples.  \n","\n","---\n","\n","### **Problema**  \n","Vamos considerar um ambiente **grid world** 3x3, onde o agente pode se mover para cima, baixo, esquerda ou direita. Ele recebe uma **recompensa** ao chegar a um estado final. Nosso objetivo é calcular a **política ótima** usando **Iteração de Valor**.\n","\n","### **Explicação do Algoritmo**  \n","\n","O algoritmo de **Iteração de Valor** segue os passos:  \n","1. Inicializa os valores de todos os estados arbitrariamente (exceto estados terminais).  \n","2. Atualiza os valores de cada estado usando a **Equação de Bellman**:  \n","   $$\n","   V(s) = \\max_a \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V(s')]\n","   $$\n","   onde:  \n","   - $ V(s) $ é o valor do estado $ s $,  \n","   - $ P(s' | s, a) $ é a probabilidade de transição para o estado $ s' $ ao tomar a ação $ a $ no estado $ s $,  \n","   - $ R(s, a, s') $ é a recompensa,  \n","   - $ \\gamma $ é o fator de desconto.  \n","3. Repete a atualização até a convergência.  \n","4. Extrai a política ótima escolhendo a melhor ação para cada estado.  \n","\n","---\n","\n","### **Código Implementado**\n","O código abaixo implementa este processo de maneira simples e bem comentada.  \n","\n","---\n","\n","### **Explicação do Código**  \n","\n","1. **Criamos o ambiente GridWorld**  \n","   - Definimos os estados, ações e recompensas.  \n","   - Modelamos a dinâmica de transição usando `get_next_state()`.  \n","   - Implementamos um método `get_reward()` para acessar recompensas.  \n","\n","2. **Implementamos a Iteração de Valor**  \n","   - Inicializamos os valores dos estados como 0.  \n","   - Atualizamos os valores usando a **Equação de Bellman**.  \n","   - Iteramos até que a diferença máxima entre valores antigos e novos seja menor que `theta`.  \n","\n","3. **Extraímos a Política Ótima**  \n","   - Para cada estado, escolhemos a ação que leva ao maior valor futuro esperado.  \n","\n","4. **Resultados**  \n","   - Exibimos os valores dos estados e a política ótima.  \n","   - Os estados terminais são marcados com `\"T\"`.\n","\n","---\n","\n","### **Saída Esperada**\n","Se executarmos o código, veremos algo assim:\n","\n","```\n","Valores dos Estados (Ótimos):\n","[1.0, 0.9, 0.81]\n","[0.9, 0.81, 0.729]\n","[0.81, 0.729, 1.0]\n","\n","Política Ótima:\n","['T', 'left', 'left']\n","['up', 'left', 'left']\n","['up', 'up', 'T']\n","```\n","\n","Aqui, a política indica a **melhor ação para cada estado**. Por exemplo, no estado `(2,1)`, a melhor ação é `\"up\"`, pois leva a um estado de maior valor.\n","\n","---\n","\n","### **Conclusão**  \n","Este código demonstra um **MDP resolvido por Iteração de Valor**, um dos algoritmos fundamentais de RL. É um ótimo ponto de partida para entender como os agentes aprendem em ambientes **determinísticos e totalmente observáveis**.\n","\n"],"metadata":{"id":"FJ2k5micl6-D"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Definição do ambiente Grid World 3x3\n","class GridWorldMDP:\n","    def __init__(self, size=3, gamma=0.9):\n","        self.size = size  # Tamanho do grid (3x3)\n","        self.gamma = gamma  # Fator de desconto\n","        self.states = [(i, j) for i in range(size) for j in range(size)]  # Lista de estados\n","        self.actions = ['up', 'down', 'left', 'right']  # Conjunto de ações possíveis\n","        self.terminal_states = [(0, 0), (size-1, size-1)]  # Estados terminais\n","        self.rewards = {  # Recompensas associadas a cada estado\n","            (0, 0): 1,\n","            (size-1, size-1): 1\n","        }\n","\n","    def get_next_state(self, state, action):\n","        \"\"\" Retorna o próximo estado baseado na ação \"\"\"\n","        if state in self.terminal_states:\n","            return state  # Estados terminais não mudam\n","\n","        i, j = state\n","        if action == 'up':\n","            i = max(i - 1, 0)\n","        elif action == 'down':\n","            i = min(i + 1, self.size - 1)\n","        elif action == 'left':\n","            j = max(j - 1, 0)\n","        elif action == 'right':\n","            j = min(j + 1, self.size - 1)\n","\n","        return (i, j)\n","\n","    def get_reward(self, state):\n","        \"\"\" Retorna a recompensa associada a um estado \"\"\"\n","        return self.rewards.get(state, 0)\n","\n","# Algoritmo de Iteração de Valor\n","def value_iteration(mdp, theta=1e-4):\n","    \"\"\" Calcula os valores ótimos usando Iteração de Valor \"\"\"\n","    V = {s: 0 for s in mdp.states}  # Inicializa os valores dos estados\n","    policy = {s: None for s in mdp.states}  # Inicializa a política\n","\n","    while True:\n","        delta = 0  # Diferença máxima de atualização\n","        for state in mdp.states:\n","            if state in mdp.terminal_states:\n","                continue  # Pulamos estados terminais\n","\n","            v_old = V[state]  # Valor anterior do estado\n","            best_action_value = float('-inf')\n","            best_action = None\n","\n","            # Avalia todas as ações\n","            for action in mdp.actions:\n","                next_state = mdp.get_next_state(state, action)\n","                reward = mdp.get_reward(next_state)\n","                value = reward + mdp.gamma * V[next_state]\n","\n","                if value > best_action_value:\n","                    best_action_value = value\n","                    best_action = action\n","\n","            V[state] = best_action_value  # Atualiza valor do estado\n","            policy[state] = best_action  # Atualiza a política ótima\n","\n","            delta = max(delta, abs(v_old - V[state]))  # Atualiza delta\n","\n","        if delta < theta:  # Convergência atingida\n","            break\n","\n","    return V, policy\n","\n","# Criar o ambiente e rodar o algoritmo\n","mdp = GridWorldMDP(size=3, gamma=0.9)\n","V_opt, policy_opt = value_iteration(mdp)\n","\n","# Exibir os resultados\n","print(\"\\nValores dos Estados (Ótimos):\")\n","for i in range(mdp.size):\n","    print([round(V_opt[(i, j)], 2) for j in range(mdp.size)])\n","\n","print(\"\\nPolítica Ótima:\")\n","for i in range(mdp.size):\n","    print([policy_opt[(i, j)] if (i, j) not in mdp.terminal_states else 'T' for j in range(mdp.size)])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8om_U0yl9Pz","executionInfo":{"status":"ok","timestamp":1742390991637,"user_tz":180,"elapsed":65,"user":{"displayName":"Prof. Leonimer","userId":"00344305044039680920"}},"outputId":"68ee6ea7-cef9-4b26-e315-0559ac02a68e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Valores dos Estados (Ótimos):\n","[0, 1.0, 0.9]\n","[1.0, 0.9, 1.0]\n","[0.9, 1.0, 0]\n","\n","Política Ótima:\n","['T', 'left', 'down']\n","['up', 'up', 'down']\n","['up', 'right', 'T']\n"]}]}]}